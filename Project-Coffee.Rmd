---
title: "Project"
author: "SeanJ- Volcaetus"
date: "4/3/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data  ##
From TidyTuesday
URL:https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-07-07


```{r }
coffee_ratings <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-07/coffee_ratings.csv')

```
# Quick overview #

```{r}
summary(coffee_ratings)
```
>A few NA's.

>1 within quakers, and 230 in Altitude low/high/mean

> Check what is happening in the rest of the data set

# Count of NA's per coloumn#
```{r}
apply(X=is.na(coffee_ratings), MARGIN = 2, FUN = sum)
```
> I will be just removing some of the columns with many missing vlaues, for instance farm_name.

```{r}
library(tidyverse)
```

Removal of columns
```{r}
coffee = coffee_ratings%>%
  select(-farm_name,-lot_number,-mill,-ico_number,-altitude,
         -altitude_low_meters,-altitude_high_meters,-producer,-company,
         -expiration,-certification_address,-owner_1,-grading_date,
         -certification_contact,-unit_of_measurement)
apply(X=is.na(coffee), MARGIN = 2, FUN = sum)
#view(coffee)
```


```{r}
coffee = na.omit(coffee)
#view(coffee)
```


```{r}
coffee[grep("lbs",coffee$bag_weight),]
```

```{r}
coffee = separate(data = coffee, col = bag_weight, into = c("weight", "type"), sep = " ")
```

```{r}
coffee$weight = as.numeric(coffee$weight)
```


```{r}
for(i in 1:length(coffee)){
  if(coffee[i,8]=="kg"){
  coffee[i,7] = round(coffee[i,7] * 2.20462,0)
  coffee[i,8] = "lbs"
  }
}  
```

```{r}
coffee = coffee%>%
  select(-type)
```

```{r}
coffee = coffee%>%
    rename(avg_altitude=altitude_mean_meters)
```

```{r}
coffee$avg_altitude = round(coffee$avg_altitude * 3.28084,0)
```


```{r}
coffee$harvest_year = substr(coffee$harvest_year,1,4)
coffee$harvest_year = as.numeric(coffee$harvest_year)
```

```{r}
summary(coffee[,c(9,12:24,26,28)])
```

```{r}
library(ggplot2)
```

check for outliers in some of the fields

```{r}
defect1_plt = ggplot(coffee, aes(y=category_one_defects)) +
              geom_boxplot()
defect2_plt = ggplot(coffee, aes(y=category_two_defects)) +
              geom_boxplot()
alt_plt = ggplot(coffee, aes(y=avg_altitude)) +
              geom_boxplot()
defect1_plt
defect2_plt
alt_plt
```

There are some outliers, but not that many that would result in a concern at this time.

Pick out some of the information that is not necessary at this point in exploration
```{r}
c = coffee[,c(1:2,4,10:26,28)]
```

Condense the data
```{r}
c.v1 = c%>%pivot_longer(
  cols = !c(species, country_of_origin,variety,processing_method,color),
  names_to = "Variables",
  values_to = "Counts")
c.v1
```

Plot the data to see overall behavior 
```{r}
ggplot(c.v1,aes(x=species,y=Counts,color=color))+geom_boxplot()+facet_wrap(~Variables,scales = "free")
```
Filter out the items that have known outliers
```{r}
c.v2 = c.v1 %>%
  filter(Variables != 'avg_altitude' & Variables != 'category_one_defects'& Variables != 'category_two_defects')
```


```{r}
ggplot(c.v2,aes(x=species,y=Counts,color=color))+geom_boxplot()+facet_wrap(~Variables,scales = "free")
```
How are the cup points distributed and where the weight it is at
```{r}
c.v2 %>%
  filter(Variables == 'total_cup_points')%>%
  ggplot(aes(x=species,y=Counts,color=color))+geom_boxplot()

c.v2 %>%
  filter(Variables == 'total_cup_points')%>%
  ggplot(aes(x=species,y=Counts,color=color))+geom_violin()
```
```{r warning=FALSE}
c.v2 %>%
  filter(Variables == 'total_cup_points')%>%
  ggplot(aes(x=processing_method,y=Counts,color=color))+geom_boxplot()+
    facet_wrap(~processing_method,scales = "free")

c.v2 %>%
  filter(Variables == 'total_cup_points')%>%
  ggplot(aes(x=processing_method,y=Counts,color=color))+geom_violin()+
    facet_wrap(~processing_method,scales = "free")
```
See the data make-up in a different view
```{r}
library(formattable)
```

```{r}
#want to make this cleaner
c.v1 %>%
  group_by(Variables,Counts) %>%
  summarise(count = n()) %>%
  mutate(freq = formattable::percent(count / sum(count)))
```

Format the label (total_cup_points) to be categorical

```{r}
coffee$tcp = coffee$total_cup_points
```

```{r}
for(i in 1:894){
  if(coffee[i,29] >= 80){
    coffee[i,29] = 80
  }
  else if(coffee[i,29] >= 70 & coffee[i,29] < 80){
    coffee[i,29] = 70
  }
  else if(coffee[i,29] >= 60 & coffee[i,29] < 70){
    coffee[i,29] = 60
  }
  else{
    coffee[i,29] = 50
  }
}
coffee$tcp = round(coffee$tcp,0)
```

Accuracy table for comparison between models
```{r}
table_accuracy = matrix(nrow=6,ncol=1)
colnames(table_accuracy) = c('Accuracy')
rownames(table_accuracy) = c('DTree','NB','SVM-Linerar','SVM-Polynomial','ANN','KNN')
table_accuracy
```
Set seed so analysis is repeatable
```{r}
set.seed(1)
```

For analysis
```{r warning=FALSE}
df = coffee[,c(9:22,25,29)]
for(i in 4 : 13){
  df[,i]=round(df[,i],2)
}
#view(df)
```


```{r}
df$processing_method= as.factor(df$processing_method)
df$variety = as.factor(df$variety)
df = df[,c(1:16)]
#view(df)
```


Simple k-fold cross validation(cv) 
```{r}
set.seed(1)
n = nrow(df)
folds = 10
tail = n%/%folds

set.seed(1)

rnd = runif(n)
rank = rank(rnd)

#block/chunck from cv is blk
blk = (rank-1)%/%tail+1
blk = as.factor(blk)

#to see formation of folds 
print(summary(blk))
```
```{r}
#df$variety = as.numeric(df$variety)
df$tcp = as.factor(df$tcp)
df$moisture = round(df$moisture,1)
```


```{r}
library(rpart)
#dtree
set.seed(1)
all.acc = numeric(0)
for(i in 1:folds){
  tree = rpart(tcp~.,df[blk != i,],method="class")
  pred = predict(tree,df[blk==i,],type="class")
  confMat = table(pred,df$tcp[blk==i])
  acc = (confMat[1,1]+confMat[2,2]+confMat[3,3]+confMat[4,4])/sum(confMat)
  all.acc = rbind(all.acc,acc)
}

print(mean(all.acc))
table_accuracy[1,1] = mean(all.acc)

```
I re-formatted the label/target field and went from a binary (good/bad) grading 
and could not figure out why the accuracy was so low (0.003) and then looked into
what the accuracy was calculating...
```{r}
confMat
```

```{r}
# naive Bayes (gaussian data)
library(e1071)
set.seed(1)

all.acc = numeric(0)
for(i in 1:folds){
  model = naiveBayes(tcp~.,df[blk != i,],method="class")
  pred = predict(model,df[blk==i,],type="class")
  confMat = table(pred,df$tcp[blk==i])
  acc = (confMat[1,1]+confMat[2,2]+confMat[3,3]+confMat[4,4])/sum(confMat)
  all.acc = rbind(all.acc,acc)
}

print(mean(all.acc))
table_accuracy[2,1] = mean(all.acc)
```


```{r}
#svm linear

set.seed(1)

all.acc = numeric(0)
for(i in 1:folds){
  model = svm(tcp~. ,df[blk != i,],kernel="linear",type="C")
  pred = predict(model,df[blk==i,],type="class")
  confMat = table(pred,df$tcp[blk==i])
  acc = (confMat[1,1]+confMat[2,2]+confMat[3,3]+confMat[4,4])/sum(confMat)
  all.acc = rbind(all.acc,acc)
}

print(mean(all.acc))
table_accuracy[3,1] = mean(all.acc)
```

```{r}
#svm poly
set.seed(1)

all.acc = numeric(0)
for(i in 1:folds){
  model = svm(tcp~.,df[blk != i,],kernel="polynomial",type="C")
  pred = predict(model,df[blk==i,],type="class")
  confMat = table(pred,df$tcp[blk==i])
  acc = (confMat[1,1]+confMat[2,2]+confMat[3,3]+confMat[4,4])/sum(confMat)
  all.acc = rbind(all.acc,acc)
}

print(mean(all.acc))
table_accuracy[4,1] = mean(all.acc)
```

```{r}
df$tcp = round(as.numeric(df$tcp),0)
```


```{r warning=FALSE}
df$tcp = as.factor(df$tcp)
#ann
library(nnet)
set.seed(1)

all.acc = numeric(0)
for(i in 1:folds){
  model = nnet(tcp~.,df[blk != i,], size = 11, trace=FALSE, rang=.06, decay=.006,maxit=500)
  pred = predict(model, df[blk==i,],type="class")
  confMat = table(factor(pred,levels=1:4),factor(df$tcp[blk==i],levels=1:4))
  acc = (confMat[1,1]+confMat[2,2]+confMat[3,3]+confMat[4,4])/sum(confMat)
  all.acc = rbind(all.acc,acc)
}
print(mean(all.acc))
table_accuracy[5,1] = mean(all.acc)
```
There was an (un)interesting issue with NN table, as it was dropping the
first two rows as it was not forward feeding into those nodes. The following
is the work around to resolve this issue.

#Before#
```{r}
set.seed(1)
i=1
  model = nnet(tcp~.,df[blk != i,], size = 10, trace=FALSE, wgts=.05)
  pred = predict(model, df[blk==i,],type="class")
  confMat = table(pred,df$tcp[blk==i])
  confMat
```
#After#
```{r}
set.seed(1)
i=1
  model = nnet(tcp~.,df[blk != i,], size = 10, trace=FALSE, wgts=.05)
  pred = predict(model, df[blk==i,],type="class")
  confMat = table(factor(pred,levels=1:4),factor(df$tcp[blk==i],levels=1:4))
  confMat
```


```{r}
df$tcp = as.factor(df$tcp)
library (caret)
trControl <- trainControl(method  = "cv", number  = 10)
```

```{r}
knn = df[,]
```

```{r warning=FALSE}
model <- train(tcp ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:10),
             trControl  = trControl,
             data       = knn)
```

```{r}
acc = mean(model$results$Accuracy)
table_accuracy[6,1] = acc
```

```{r}
tab = round(table_accuracy,4)
tab
```
